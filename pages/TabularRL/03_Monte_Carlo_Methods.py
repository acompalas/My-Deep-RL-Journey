import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go


st.title("Monte Carlo Methods")

section = st.selectbox("", [
    "Overview",
    "State-Value Estimation",
    "State-Action Value Estimation",
    "Importance Sampling",
])

if section == "Overview":
    st.title("Overview")
    
    st.markdown(r"""
                
    ## Topics
    - Value estimation using sample averaging
        - First-visit Monte Carlo prediction
    - State-Action Values
        - $\epsilon$-greedy on-policy MC control
    
    - Importance Sampling
        - Off-Policy  MC control
    
    ## Learning Objectives
    
    - Estimate state values using Monte Carlo prediction method
    - Estimate state-action values using $\epsilon$-greedy Monte Carlo control and use them to learn or improve polices
    - Use importance sampling to estimate values of different policies
    - Implement off-policy MC control to learn optimal policies   
    
    ## Learning from Experience
    
    ### **Problem**: 
    
    Dynamic Programming requires knowledge of the environment model. 
    
    Evaluation of the Bellman update equations requires that we know the problem parameters
    namely the **transition** and **reward** functions.
    
    But in many problems these are often inaccessible or difficult to compute.
    
    ### **Goal**:
    
    In **Reinforcement Learning** we want to make decisions from what we learn from actual experience.
    
    Instead of starting out with all of the problem's information, sitting still and thinking really hard with dynamic programming, a reinforcement learning agent
    would learn by interacting with the environment, receiving rewards, and formulating policies.
    
    ## Monte Carlo Methods
    
    ### **Summary**
    
    Generate many samples representing experience in the decision problem, and then avereage what we have seen to learn about different states and actions. 
    
    
    Recall the definition of the value function for a fixed policy $\pi$:
    $$
    V^{\pi}(s) = E\left[\sum_{t=0}\gamma^t R(s_t, \pi(s_t), s_{t+1})\right]
    $$
    
    The value of the state $s$ is defined by the expectation of the discounted sum of reward while following a policy $\pi$.
    
    ### Problem:
    
    This expectation cannot be observed directly by a single stream of rewards received by the agent.
    
    ### Idea:
    
    The idea with Monte Carlo methods is to use sample averaging to approximate the expectation part of the value equation by averaging utility over multiple episodes.
    
    ## Conclusion
    
    - Monte Carlo Methods learn value functions and policies from experience
    - Useful when model is unknown or difficult to compute
    - Useful for focusing on subset of the state space
    
    - MC Prediction averages utilities from episode sequences to evaluate policies
    - Exploration is required for MC control and learning new policies
    
    - On-policy methods such as $\epsilon$-greedy optimize policies that maintain exploration
    - Off-policy methods use importance sampling to transform utilities generated by a behaviour policy to estimate those of a target policy.
    """)
    
    
    
if section == "State-Value Estimation":
    st.title("State-Value Estimation")
    st.markdown(r"""
             
    ## Idea   
    Imagine our agent has the ability to execute many decision making processes starting from s receiving rewards each time according to the (unknown) transition seen.
    
    Each episode yields a utility estimate for $V(s)$ and the overall average can then be computed.    
    
    The idea of state estimation is that the value function $V(s)$ can be estimated by averaging utilities observed after visiting state $s$.
    
    We take $G(s)$ to be the state utility estimate of state $s$.
    
    $$
    V(s) = \frac{1}{N} \sum_{i=1}^N G_i(s)
    $$
    
    of s. Once we have gathered information from a suitable number of episodes, $V(s)$ will then be the simple average of all $G(s)$
    
    ## First-Visit MC Prediction
    
    First-Visit MC: A value is estimated after first visit to state within episode.
    
    Within an episode, the same state can be visited multiple times.
    
    First Visit MC will estimate the utility of a state starting from the first visit to that state.
    
    ## Algorithm
    - Initialize $V^{\pi}(s), Count(s) \leftarrow 0$ for each state $\in S$
    - Loop:
        - Generate episode $E$ following policy $\pi$ starting from some random initial state to a terminal state: 
        $$
        s_0, a_0, r_1, s_1, a_1, r_2..., s_{T-1}, a_{T-1}, r_T
        $$
        - For first occurrence of each state $s_t \in E$ calculate the sample value seen after first visiting $s_t$
        $$
        G \leftarrow \sum_{j=0}^{T-t-1}\gamma^j r_{j+t+1}
        $$
        - Average the new sample average into the value function for policy $\pi$ using a moving average formula
        $$
        V^{\pi}(s_t) \leftarrow \frac{(Count(s_t) * V^\pi(s_t) + G)}{(Count(s_t) + 1)}
        $$
        - Increment Count
        $$
        Count(s_t) \leftarrow Count(s_t) + 1
        $$
        
    We expect that after running this algorithm for a sufficient number of episodes we will start to see our value function approach the true value function
    
    ## Finer Points
    
    The whole point of sampling is that we just take what we see in each episode.
    
    Other than the starting state, we can't control which other states we see.
    
    Naturally, more states will be visited more than others.
    
    The guarantee that we do have is that as we visit a state more often, the value that we estimate for it will converge to its true value.
    
    ### Differences to Dynamic Programming
    
    One major difference from dynamic programming is that thte value estimates of different states are independent of each other. 
    
    In value or policy iteration, the computation of $V(s)$ requires that we have an estimate for successor values $V(s')$
    
    In contrast, Monte Carlo prediction of $V(s)$ only depends on the rewards seen, which are not estimates.
    
    Thus, the computational complexity of Monte Carlo prediction is independent of the size of the state space.
    
    Hence, it is possible to get a good estimate for a particular state value without waiting for all other state values to converge.
    
      
    """)
    
    

if section == "State-Action Value Estimation":
    st.title("State-Action Value Estimation")
    st.markdown(r"""
    If we want to improve upon a policy using Monte Carlo methods we no longer have the transition function and exact reward function in the Bellman update function for policy iteration.
    
    $$
    \pi(s) \leftarrow \text{argmax}_a \sum_{s'} T(s, a, s')[R(s, a, s') + \gamma V^\pi (s')]
    $$
    
    Instead we attempt to learn the state-action values directly rather than computing them.
                
    $$
    \pi(s) \leftarrow \text{argmax}_a Q(s,a)
    $$
    
    $Q(s,a)$ is the expected utitlity of s after taking an action $a$ and then following $\pi$ thereafter and are called **state-action** values
    
    ## Q Values, State Values, and Policy
    
    Q values can be defined as functions of $V^*$ or recursively as functions of themselves.
    
    $$
    Q(s, a) =  \sum_{s'} T(s, a, s')[R(s, a, s') + \gamma V^* (s')]\\
            = \sum_{s'} T(s, a, s')[R(s, a, s') + \gamma \text{max}_{a'} Q(s', a')]
    $$
    
    To learn policies using reinforcement learning, we will first learn Q values then use these to compute the optimal value and policy functions $V^*$ or $\pi^*$
    $$
    V^*(s) = \text{max}_a Q(s, a)
    $$
    
    $$
    \pi^*(s) = \text{argmax}_a Q(s, a)
    $$
    
    ## $\epsilon$-Greedy On-Policy MC Control
    
    ### Goal
    
    Learn optimal or close to optimal policy via Monte Carlo RL
    
    ### Idea
    
    Using the idea from policy iteration, we will alternate between evaluation and improvment
    
    Given some starting policy, we will use sampling to find the associated state-action value function.
    
    Then we will use the value function to iteratively improve the policy by selecting new actions greedily
    
    $$
    \pi_{i+1}(s) = \text{argmax}_a Q^{\pi_i}(s,a)
    $$
    
    ## $\epsilon$-Greedy Policies
    
    However we want to explore the state-action space so we allow for some exploration over greedy selection.
    
    Choose best action most of the time, but with small probability $\epsilon$, execute a random action instead
    
    $$
    \pi(a|s) = \begin{cases} 1- \epsilon \quad \text{for} \  a = \text{argmax}_{a'} Q(s, a') \\ \frac{\epsilon}{|A(s)| - 1} \quad \text{for all other actions} \ a\end{cases}
    $$
    
    ## Algorithm
    
    - Initialize $Q^\pi(s, a) \leftarrow 0$, $Count(s, a) \leftarrow 0$, for all $s \in S$, for all $a \in A$
    - Initialize $\epsilon$, $\pi$
    - Loop:
        - Generate new episode $E$ following $\pi$: 
        $$
        s_0, a_0, r_1, s_1, a_1, r_2, ..., s_{T-1}, a_{T-1}, r_T
        $$  
        - For first occurrence of each pair $(s_t, a_t) \in E$:
        
            - Calculate estimated utility for episode.
        
            $$
            G \leftarrow \sum_{j=0}^{T-t-1} \gamma^j r_{j+t+1}
            $$
            - Incorporate into Q value update formula
        
            $$
            Q^\pi (s_t, a_t) \leftarrow \frac{(Count(s_t, a_t) * Q^\pi(s_t, a_t) + G)}{Count(s_t, a_t) + 1}
            $$
            
            - Increment count
    
    """)
    
if section == "Importance Sampling":
    st.title("Importance Sampling")
    st.markdown(r"""
    ## On-Policy Learning
    
    ### Problem:
    
    Suppose we run $\epsilon$-greedy MC control until our $Q$-values converge.
    
    However, $\epsilon$-greedy MC control doesn't actually give us $Q$ and $\pi^*$
    
    Since $\epsilon > 0$, the state-action values and policy converge under that condition
    
    We've only learned the values and policy that best describe the specific problem.
    
    This is called on-policy learning where we learn the values exactly for the policy that the agent is following.
    
    ### On-Policy Learning
    
    Learn the values for the specific policy we are following
    
    If we wanted the optimal policy, we need to decrease $\epsilon$ over time.
    
    ### Off-Policy Learning
    
    Use the behaviour policy $b$ to learn about a different target policy $\pi$
    
    - More general than on-policy learning, but slower to converge.
    
    Off-Policy Prediction: Estimat values for $\pi$ using episodes from $b$
    
    - We requires that all state-actions allowed by $\pi$ are also allowed by $b$
    
    $$
    \pi(a_j| s_j) > 0 \implies b(a_j|s_j) > 0
    $$
    
    ## Importance Sampling
    
    Suppose we observe an episode from behaviour policy $b$:
    
    We define the **importance sampling ratio** of a transition at time $t$ as the relative likelihood that all subsequence state-action pairs occur in the target versus the behaviour policies.
    
    $$
    w_t = \prod_{j=t}^{T-1} \frac{\pi(a_j|s_j)}{b(a_j|s_j)}
    $$
    
    The numerator is the product of all state-action probabilities under $\pi$
    
    The denominator is the product of all state-action probabilties under $b$
         
    We can use this weight to transform to transform utitly values of a state-action pair under b as into one generated by $\pi$
    
    ### Weighted Averages
    
    $$
    Q(s_t, a_t) = E[w_{t+1}G_t]
    $$
    
    When updating $Q(s_t, a_t)$ with estimates from multiple episodes, we compute the weighted average by dividing by the sum of all weights.
    
    ### Algorithm: Off Policy-MC control with Weighted Importance Sampling
    - Initialize $Q^\pi(s, a) \leftarrow 0$, $W(s, a) \leftarrow 0$, for all $s \in S$ and $a \in A$
    - Initialize $b \leftarrow \epsilon$-greedy behaviour policy, $\pi \leftarrow$ greedy policy
    
    - Loop:
        - Generate new episode $E$ following $b$:
        - For first occurence of each pair $(s_t, a_t) \in E$:
        $$
        G \leftarrow \sum_{j=0}^{T-t-1} \gamma^j r_{j+t+1}, \quad w_t \leftarrow \prod_{j=t}^{T-1} \frac{\pi(a_j|s_j)}{b(a_j|s_j)}
        $$
        
        $$
        Q^\pi(s_t, a_t) \leftarrow \frac{(W(s_t, a_t) * Q^\pi(s_t, a_t) + wG)}{W(s_t, a_t) + w)}
        $$
    
        $$
        W(s_t, a_t) \leftarrow W(s_t, a_t) + w
        $$
        
        $$
        \pi(s_t) \leftarrow \text{argmax}_a Q^\pi(s, a)
        $$
    
    """)
    